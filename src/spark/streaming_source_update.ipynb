{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Spark Structured Streaming - ignoreChanges\n",
    "\n",
    "This notebook demos what happens when Spark Structured Streaming data source is overwritten/updated."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/13 09:04:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"x\")\n",
    "         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "         .getOrCreate())\n",
    "\n",
    "def stop_streaming_query_gracefully(streaming_query: StreamingQuery):\n",
    "    query_status = streaming_query.status[\"message\"]\n",
    "    while query_status != \"Stopped\":\n",
    "        print(f\"{streaming_query.name} status: {query_status}\")\n",
    "        time.sleep(3)\n",
    "        query_status = streaming_query.status[\"message\"]\n",
    "        print(f\"{streaming_query.name} status: {query_status}\")\n",
    "    streaming_query.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id|date|value|\n",
      "+---+----+-----+\n",
      "|{1}|   0|    a|\n",
      "+---+----+-----+\n",
      "\n",
      "root\n",
      " |-- id: struct (nullable = true)\n",
      " |    |-- id: integer (nullable = true)\n",
      " |-- date: integer (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(id.id=1)]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StructType([StructField(\"id\", IntegerType())])),\n",
    "    StructField(\"date\", IntegerType()),\n",
    "    StructField(\"value\", StringType())\n",
    "])\n",
    "\n",
    "raw_0 = spark.createDataFrame(data=[[{\"id\": 1}, 0, \"a\"]], schema=schema)\n",
    "raw_0.write.format(\"delta\").save(\"delta/raw\")\n",
    "spark.read.format(\"delta\").load(\"delta/raw\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.streaming.StreamingQuery at 0x7f25a877fbd0>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_raw = spark.readStream.format(\"delta\").load(\"delta/raw\")\n",
    "stream_raw.writeStream.queryName(\"raw_memory\").format(\"memory\").option(\"checkpointLocation\", \"delta/checkpoint/memory\").trigger(once=True).start()\n",
    "from time import sleep\n",
    "sleep(4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```-------------------------------------------\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+---+---+\n",
    "|  a|  b|\n",
    "+---+---+\n",
    "|  1|  1|\n",
    "+---+---+\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.streaming.StreamingQuery at 0x7f25a87f3150>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_0 = spark.readStream.format(\"delta\").load(\"delta/raw\")\n",
    "err_0.writeStream.format(\"console\").option(\"checkpointLocation\", \"delta/checkpoint/two_err\").trigger(once=True).start()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```-------------------------------------------\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+---+---+\n",
    "|  a|  b|\n",
    "+---+---+\n",
    "|  1|  1|\n",
    "+---+---+\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/12 12:47:26 WARN MicroBatchExecution: The read limit MaxFiles: 1000 for DeltaSource[file:/home/delta/one] is ignored when Trigger.Once() is used.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pyspark.sql.streaming.StreamingQuery at 0x7f25b0088b90>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_0 = spark.readStream.format(\"delta\").option(\"ignoreChanges\", \"true\").load(\"delta/raw\")\n",
    "two_0.writeStream.format(\"delta\").option(\"checkpointLocation\", \"delta/checkpoint/two\").trigger(once=True).start(\"delta/two\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"delta/two\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.streaming.StreamingQuery at 0x7f25a877f710>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/12 12:47:50 WARN MicroBatchExecution: The read limit MaxFiles: 1000 for DeltaSource[file:/home/delta/one] is ignored when Trigger.Once() is used.\n"
     ]
    }
   ],
   "source": [
    "three_0 = spark.readStream.format(\"delta\").option(\"ignoreChanges\", \"true\").load(\"delta/raw\")\n",
    "three_0 = three_0.dropDuplicates([\"a\"])\n",
    "three_0.writeStream.format(\"delta\").option(\"checkpointLocation\", \"delta/checkpoint/three\").trigger(once=True).start(\"delta/three\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"delta/three\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overwrite source table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  2|  4|\n",
      "|  1|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = spark.createDataFrame(data=[[1,2],[2,4]], schema=\"a int, b int\")\n",
    "df_1.write.format(\"delta\").mode(\"overwrite\").save(\"delta/raw\")\n",
    "spark.read.format(\"delta\").load(\"delta/raw\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.streaming.StreamingQuery at 0x7f25a8787e10>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "console_1 = spark.readStream.format(\"delta\").option(\"ignoreChanges\", \"true\").load(\"delta/raw\")\n",
    "console_1.writeStream.format(\"console\").option(\"checkpointLocation\", \"delta/checkpoint/two_console\").trigger(once=True).start()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+---+---+\n",
    "|  a|  b|\n",
    "+---+---+\n",
    "|  1|  2|\n",
    "|  2|  4|\n",
    "+---+---+\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.streaming.StreamingQuery at 0x7f25a877f350>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/12 12:48:23 WARN MicroBatchExecution: The read limit MaxFiles: 1000 for DeltaSource[file:/home/delta/one] is ignored when Trigger.Once() is used.\n"
     ]
    }
   ],
   "source": [
    "err_1 = spark.readStream.format(\"delta\").load(\"delta/raw\")\n",
    "err_1.writeStream.format(\"console\").option(\"checkpointLocation\", \"delta/checkpoint/two_err\").trigger(once=True).start()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Error message\n",
    "```\n",
    "java.lang.UnsupportedOperationException: Detected a data update (for example part-00003-a001073f-99a3-4da2-894f-29f1dd1943e6-c000.snappy.parquet) in the source table at version 1.\n",
    "This is currently not supported. If you'd like to ignore updates, set the option 'ignoreChanges' to 'true'. If you would like the data update to be reflected,\n",
    "please restart this query with a fresh checkpoint directory.\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.streaming.StreamingQuery at 0x7f7fc25dab90>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/12 12:33:24 WARN MicroBatchExecution: The read limit MaxFiles: 1000 for DeltaSource[file:/home/delta/one] is ignored when Trigger.Once() is used.\n"
     ]
    }
   ],
   "source": [
    "err_2 = spark.readStream.format(\"delta\").load(\"delta/raw\")\n",
    "err_2.writeStream.format(\"console\").option(\"checkpointLocation\", \"delta/checkpoint/two_err_2\").trigger(once=True).start()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After change the checkpoint location, the new stream reader reads the newly overwritten table `raw`.\n",
    "Note the batch number is 0, instead of 1.\n",
    "```\n",
    "-------------------------------------------\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+---+---+\n",
    "|  a|  b|\n",
    "+---+---+\n",
    "|  1|  2|\n",
    "|  2|  4|\n",
    "+---+---+\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/12 12:33:11 WARN MicroBatchExecution: The read limit MaxFiles: 1000 for DeltaSource[file:/home/delta/one] is ignored when Trigger.Once() is used.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pyspark.sql.streaming.StreamingQuery at 0x7f7fc1d45910>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_1 = spark.readStream.format(\"delta\").option(\"ignoreChanges\", \"true\").load(\"delta/raw\")\n",
    "two_1.writeStream.format(\"delta\").option(\"checkpointLocation\", \"delta/checkpoint/two\").trigger(once=True).start(\"delta/two\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}