{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dateutil.tz import gettz\n",
    "import datetime\n",
    "from typing import Tuple\n",
    "from pyspark.sql import SparkSession, DataFrame, functions, Window\n",
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class StockETL:\n",
    "    \"\"\"Extract, transform and load xetra data into DeltaLake.\"\"\"\n",
    "\n",
    "    def __init__(self, extraction_mode: str, month_to_extract: str = None):\n",
    "        \"\"\"Instantiate StockETL.\n",
    "\n",
    "        Args:\n",
    "            extraction_mode: data extracting mode, \"full\", \"incremental\", or \"month\". \"full\" mode\n",
    "              extracts all the CSV files from the bucket, \"incremental\" extracts CSVs from yesterday,\n",
    "              \"month\" extracts CSV from a given month.\n",
    "            month_to_extract: the target month from which the CSV files should be extracted.\n",
    "              Required when extraction_mode is \"month\". The value should be a given month, in format\n",
    "              \"YYYY-MM\", for example 2022 January would be \"2022-01\".\n",
    "        \"\"\"\n",
    "        if extraction_mode.lower() not in [\"full\", \"incremental\", \"month\"]:\n",
    "            raise ValueError('Unrecognized extraction mode. '\n",
    "                             'The extraction mode should be \"full\", \"month\" or \"incremental\".')\n",
    "        else:\n",
    "            self.extraction_mode = extraction_mode.lower()\n",
    "\n",
    "        if extraction_mode == \"month\" and month_to_extract is None:\n",
    "            raise ValueError('Parameter \"month_to_load\" is required '\n",
    "                             'when using extracting mode \"month\".')\n",
    "        else:\n",
    "            self.month_to_extract = month_to_extract\n",
    "\n",
    "        self.spark = self.configure_spark_session()\n",
    "\n",
    "    @staticmethod\n",
    "    def configure_spark_session() -> SparkSession:\n",
    "        \"\"\"Create a SparkSession.\n",
    "\n",
    "        The session is configured to access public S3 bucket anonymously, read more\n",
    "        https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Anonymous_Login_with_AnonymousAWSCredentialsProvider,\n",
    "        and use DeltaLake engine. The log level is set to ERROR to reduce noise.\n",
    "\n",
    "        Returns:\n",
    "            A configured SparkSession\n",
    "        \"\"\"\n",
    "        spark = (SparkSession.builder.appName(\"xetra\")\n",
    "                 .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "                 .config(\"spark.sql.catalog.spark_catalog\",\n",
    "                         \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "                 .config(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "                 .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "                         \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "                 .getOrCreate())\n",
    "        spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "        return spark\n",
    "\n",
    "    def extract_xetra_data(self) -> DataFrame:\n",
    "        \"\"\"Extract xetra CSVs from S3 bucket deutsche-boerse-xetra-pds into a Spark DataFrame.\n",
    "\n",
    "        xetra data schema can be found here\n",
    "         https://github.com/Deutsche-Boerse/dbg-pds/blob/master/API_README.md#xetra.\n",
    "\n",
    "        Returns:\n",
    "            A Spark DataFrame of xetra stock data.\n",
    "        \"\"\"\n",
    "        schema = (StructType()\n",
    "                  .add(\"isin\", \"string\")\n",
    "                  .add(\"mnemonic\", \"string\")\n",
    "                  .add(\"securityDesc\", \"string\")\n",
    "                  .add(\"securityType\", \"string\")\n",
    "                  .add(\"currency\", \"string\")\n",
    "                  .add(\"securityID\", \"long\")\n",
    "                  .add(\"date\", \"date\")\n",
    "                  .add(\"time\", \"string\")\n",
    "                  .add(\"startPrice\", \"float\")\n",
    "                  .add(\"maxPrice\", \"float\")\n",
    "                  .add(\"minPrice\", \"float\")\n",
    "                  .add(\"endPrice\", \"float\")\n",
    "                  .add(\"tradedVolume\", \"float\")\n",
    "                  .add(\"numberOfTrades\", \"integer\"))\n",
    "\n",
    "        s3_base_uri = \"s3a://deutsche-boerse-xetra-pds\"\n",
    "        if self.extraction_mode == \"full\":\n",
    "            s3_uri = f\"{s3_base_uri}/**/\"\n",
    "        elif self.extraction_mode == \"incremental\":\n",
    "            yesterday = (datetime.datetime.now(gettz('Berlin'))\n",
    "                         - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "            s3_uri = f\"{s3_base_uri}/{yesterday}/\"\n",
    "        else:\n",
    "            s3_uri = f\"{s3_base_uri}/{self.month_to_extract}*/\"\n",
    "\n",
    "        return self.spark.read.csv(s3_uri, schema=schema, header=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_datetime_column(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Return a new dataframe with column \"datetime\" added.\n",
    "\n",
    "        Column \"datetime\" is of data type \"timestamp\". The column is required to sort xetra trading\n",
    "        records.\n",
    "\n",
    "        Args:\n",
    "            df: xetra Spark DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            A Spark DataFrame\n",
    "        \"\"\"\n",
    "        return (df.withColumn(\"datetime\",\n",
    "                              functions.to_timestamp(functions.concat_ws(\" \", df.date, df.time))))\n",
    "\n",
    "    def get_opening_and_closing_prices(self, df: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
    "        \"\"\"Return two DataFrames with daily opening and closing prices for all trading securities.\n",
    "\n",
    "        Args:\n",
    "            df: A xetra Spark DataFrame\n",
    "\n",
    "        Returns:\n",
    "            A tuple of two DataFrames, first hold opening price info, the second holds the closing\n",
    "              price info.\n",
    "        \"\"\"\n",
    "        df_i = self._create_datetime_column(df)\n",
    "\n",
    "        asc_window = Window.partitionBy([\"isin\", \"date\"]).orderBy(df_i.datetime.asc())\n",
    "        start_df = (df_i.withColumn(\"asc_rank\", functions.rank().over(asc_window))\n",
    "                    .where(\"asc_rank = 1\")\n",
    "                    .drop(\"asc_rank\"))\n",
    "\n",
    "        desc_window = Window.partitionBy([\"isin\", \"date\"]).orderBy(df_i.datetime.desc())\n",
    "        end_df = (df_i.withColumn(\"desc_rank\", functions.rank().over(desc_window))\n",
    "                  .where(\"desc_rank = 1\")\n",
    "                  .drop(\"desc_rank\"))\n",
    "\n",
    "        return start_df, end_df\n",
    "\n",
    "    def get_intra_day_performance(self, opening: DataFrame, closing: DataFrame):\n",
    "        \"\"\" Get daily performance for each security.\n",
    "\n",
    "        Determine security performance by joining opening price with closing price.\n",
    "\n",
    "        Note that the current day data are dropped, because the complete CSVs will only be available\n",
    "        after the current day.\n",
    "\n",
    "        Args:\n",
    "            opening: A DataFrame with security daily opening price info\n",
    "            closing: A DataFrame with security daily closing price info\n",
    "\n",
    "        Returns:\n",
    "            A DataFrame of security daily performance. Each security has one row per trading day.\n",
    "        \"\"\"\n",
    "        df = (opening.alias(\"start\")\n",
    "                        .join(closing.alias(\"end\"), on=[\"isin\", \"date\"])\n",
    "                        .select(\"isin\", \"date\", \"start.startPrice\", \"end.endPrice\"))\n",
    "        if self.extraction_mode in [\"full\", \"month\"]:\n",
    "            today = datetime.datetime.now(gettz('Berlin')).strftime(\"%Y-%m-%d\")\n",
    "            df = df.where(f\"date != '{today}'\")\n",
    "        return (df\n",
    "                .withColumn(\"performance\", (df.endPrice - df.startPrice) / df.startPrice))\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_partition_column(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Return a new dataframe with column \"partition\" added.\n",
    "\n",
    "        Column \"partition\" is of data type \"string\". The column is used to write the xetra dataframe to\n",
    "        a Delta table out in partition.\n",
    "\n",
    "        Args:\n",
    "            df: xetra Spark DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            A Spark DataFrame\n",
    "        \"\"\"\n",
    "        return df.withColumn(\"partition\", functions.date_format(\"date\", \"yyyy-MM\"))\n",
    "        # .select(\"isin\", \"datetime\", \"date\", \"startPrice\", \"endPrice\", \"partition\")\n",
    "\n",
    "    def load_delta_table_stock_performance(self, df: DataFrame) -> None:\n",
    "        \"\"\"Load dataframe into Delta table.\n",
    "\n",
    "        Args:\n",
    "            df: A spark dataframe to write out\n",
    "        \"\"\"\n",
    "        df_i = self._create_partition_column(df)\n",
    "\n",
    "        if self.extraction_mode == \"incremental\":\n",
    "            output_mode = \"append\"\n",
    "        else:\n",
    "            output_mode = \"overwrite\"\n",
    "\n",
    "        (df_i.coalesce(1) # to reduce number of parquet files\n",
    "         .write\n",
    "         .format(\"delta\")\n",
    "         .partitionBy(\"partition\")\n",
    "         .mode(output_mode)\n",
    "         .saveAsTable(\"stock_performance\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initial Full ETL\n",
    "Historical xetra data are extracted from S3 bucket, transformed and loaded as Delta table\n",
    "`stock_performance`.\n",
    "\n",
    "Initial full ETL only needs to be run once.\n",
    "\n",
    "The full ETL processes data until `today - 1 day`, today's data will be processed tomorrow by\n",
    "the incremental ETL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/15 01:04:19 WARN Utils: Your hostname, LMAC-XIATONGZHENG.local resolves to a loopback address: 127.0.0.1; using 192.168.178.34 instead (on interface en0)\n",
      "22/02/15 01:04:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/zheng/git_projects/mop_hamster/venv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/15 01:04:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/15 01:04:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "etl = StockETL(extraction_mode=\"month\", month_to_extract=\"2022-02\")\n",
    "raw_df = etl.extract_xetra_data()\n",
    "# raw_df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================================> (51 + 1) / 52]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+--------+--------------------+\n",
      "|        isin|      date|startPrice|endPrice|         performance|\n",
      "+------------+----------+----------+--------+--------------------+\n",
      "|DE0005493365|2022-02-01|     394.0|   408.4|  0.0365482078590974|\n",
      "|DE0005493365|2022-02-02|     411.6|   398.8|-0.03109819757225107|\n",
      "|DE0005493365|2022-02-03|     396.0|   386.0|-0.02525252525252...|\n",
      "|DE0005493365|2022-02-04|     386.0|   382.8|-0.00829018706484...|\n",
      "|DE0005493365|2022-02-07|     385.0|   374.2|-0.02805191634537...|\n",
      "|DE0005493365|2022-02-08|     385.0|   370.8|-0.03688314858969156|\n",
      "|DE0005493365|2022-02-09|     373.2|   383.2|0.026795283153561472|\n",
      "|DE0005493365|2022-02-10|     381.8|   376.0|-0.01519116809431067|\n",
      "|DE0005493365|2022-02-11|     371.2|   372.8|0.004310278915198789|\n",
      "|DE0005493365|2022-02-14|     358.0|   365.2| 0.02011176594142807|\n",
      "+------------+----------+----------+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "opening_price_df, closing_price_df = etl.get_opening_and_closing_prices(df=raw_df)\n",
    "performance_df = etl.get_intra_day_performance(opening=opening_price_df, closing=closing_price_df)\n",
    "performance_df.where(\"isin = 'DE0005493365'\").sort(\"date\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "etl.load_delta_table_stock_performance(performance_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Daily Incremental ETL\n",
    "After running the full ETL once, the incremental ETL should be scheduled every day to process data\n",
    "from yesterday."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "incremental_etl = StockETL(extraction_mode=\"incremental\")\n",
    "yesterday_raw_df = incremental_etl.extract_xetra_data()\n",
    "start_df, end_df = incremental_etl.get_opening_and_closing_prices(yesterday_raw_df)\n",
    "yesterday_performance_df = incremental_etl.get_intra_day_performance(start_df, end_df)\n",
    "incremental_etl.load_delta_table_stock_performance(yesterday_performance_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+--------+--------------------+---------+\n",
      "|        isin|      date|startPrice|endPrice|         performance|partition|\n",
      "+------------+----------+----------+--------+--------------------+---------+\n",
      "|AT0000652011|2022-02-07|     43.04|   43.36|0.007434936989250247|  2022-02|\n",
      "|AT0000946652|2022-02-09|     39.35|   39.75|0.010165223415242852|  2022-02|\n",
      "|BE0974293251|2022-02-03|     56.15|   54.51|-0.02920753623826...|  2022-02|\n",
      "|CH0514065058|2022-02-08|      0.86|  0.8829|0.026627889456395947|  2022-02|\n",
      "|CH1102728750|2022-02-08|    21.556|  20.548|-0.04676189606524838|  2022-02|\n",
      "|DE0005493365|2022-02-07|     385.0|   374.2|-0.02805191634537...|  2022-02|\n",
      "|DE0005878003|2022-02-07|     41.95|   42.15|0.004767598553088536|  2022-02|\n",
      "|DE0006569908|2022-02-02|      8.28|    8.17|-0.01328498311892...|  2022-02|\n",
      "|DE0007551509|2022-02-10|      85.0|    85.0|                 0.0|  2022-02|\n",
      "|DE0007846867|2022-02-09|     11.65|    11.7|0.004291862006187013|  2022-02|\n",
      "|DE000A0BVU28|2022-02-11|      24.2|    24.0|-0.00826449407579...|  2022-02|\n",
      "|DE000A0BVU28|2022-02-14|      23.6|    23.4|-0.00847460846214604|  2022-02|\n",
      "|DE000A0D6554|2022-02-03|     14.05|   13.47|-0.04128113279945...|  2022-02|\n",
      "|DE000A0F5UH1|2022-02-02|     30.75|   30.69|-0.00195120214446...|  2022-02|\n",
      "|DE000A0F5UH1|2022-02-11|     30.84|   30.96|0.003891015930319...|  2022-02|\n",
      "|DE000A0HGQS8|2022-02-14|      2.24|     2.2|-0.01785712575121809|  2022-02|\n",
      "|DE000A0KFUJ5|2022-02-07|       6.4|     6.4|                 0.0|  2022-02|\n",
      "|DE000A0KRJ28|2022-02-10|    4.9592|  4.9396|-0.00395223320791262|  2022-02|\n",
      "|DE000A0KRKB8|2022-02-09|    5.6385|  5.6765| 0.00673931516089247|  2022-02|\n",
      "|DE000A0LBFE4|2022-02-08|      32.6|    32.6|                 0.0|  2022-02|\n",
      "+------------+----------+----------+--------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = StockETL.configure_spark_session()\n",
    "perf_df = spark.table(\"stock_performance\")\n",
    "perf_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark ="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}