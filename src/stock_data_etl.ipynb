{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dateutil.tz import gettz\n",
    "import datetime\n",
    "from typing import Tuple\n",
    "from pyspark.sql import SparkSession, DataFrame, functions, Window\n",
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class StockETL:\n",
    "    \"\"\"Extract, transform and load xetra data into DeltaLake.\"\"\"\n",
    "\n",
    "    def __init__(self, extraction_mode: str, month_to_extract: str = None):\n",
    "        \"\"\"Instantiate StockETL.\n",
    "\n",
    "        Args:\n",
    "            extraction_mode: data extracting mode, \"full\", \"incremental\", or \"month\". \"full\" mode\n",
    "              extracts all the CSV files from the bucket, \"incremental\" extracts CSVs from yesterday,\n",
    "              \"month\" extracts CSV from a given month.\n",
    "            month_to_extract: the target month from which the CSV files should be extracted.\n",
    "              Required when extraction_mode is \"month\". The value should be a given month, in format\n",
    "              \"YYYY-MM\", for example 2022 January would be \"2022-01\".\n",
    "        \"\"\"\n",
    "        if extraction_mode.lower() not in [\"full\", \"incremental\", \"month\"]:\n",
    "            raise ValueError('Unrecognized extraction mode. '\n",
    "                             'The extraction mode should be \"full\", \"month\" or \"incremental\".')\n",
    "        else:\n",
    "            self.extraction_mode = extraction_mode.lower()\n",
    "\n",
    "        if extraction_mode == \"month\" and month_to_extract is None:\n",
    "            raise ValueError('Parameter \"month_to_load\" is required '\n",
    "                             'when using extracting mode \"month\".')\n",
    "        else:\n",
    "            self.month_to_extract = month_to_extract\n",
    "\n",
    "        self.spark = self.configure_spark_session()\n",
    "\n",
    "    @staticmethod\n",
    "    def configure_spark_session() -> SparkSession:\n",
    "        \"\"\"Create a SparkSession.\n",
    "\n",
    "        The session is configured to access public S3 bucket anonymously, read more\n",
    "        https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Anonymous_Login_with_AnonymousAWSCredentialsProvider,\n",
    "        and use DeltaLake engine. The log level is set to ERROR to reduce noise.\n",
    "\n",
    "        Returns:\n",
    "            A configured SparkSession\n",
    "        \"\"\"\n",
    "        spark = (SparkSession.builder.appName(\"xetra\")\n",
    "                 .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "                 .config(\"spark.sql.catalog.spark_catalog\",\n",
    "                         \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "                 .config(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "                 .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "                         \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "                 .getOrCreate())\n",
    "        spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "        return spark\n",
    "\n",
    "    def extract_xetra_data(self) -> DataFrame:\n",
    "        \"\"\"Extract xetra CSVs from S3 bucket deutsche-boerse-xetra-pds into a Spark DataFrame.\n",
    "\n",
    "        xetra data schema can be found here\n",
    "         https://github.com/Deutsche-Boerse/dbg-pds/blob/master/API_README.md#xetra.\n",
    "\n",
    "        Returns:\n",
    "            A Spark DataFrame of xetra stock data.\n",
    "        \"\"\"\n",
    "        schema = (StructType()\n",
    "                  .add(\"isin\", \"string\")\n",
    "                  .add(\"mnemonic\", \"string\")\n",
    "                  .add(\"securityDesc\", \"string\")\n",
    "                  .add(\"securityType\", \"string\")\n",
    "                  .add(\"currency\", \"string\")\n",
    "                  .add(\"securityID\", \"long\")\n",
    "                  .add(\"date\", \"date\")\n",
    "                  .add(\"time\", \"string\")\n",
    "                  .add(\"startPrice\", \"float\")\n",
    "                  .add(\"maxPrice\", \"float\")\n",
    "                  .add(\"minPrice\", \"float\")\n",
    "                  .add(\"endPrice\", \"float\")\n",
    "                  .add(\"tradedVolume\", \"float\")\n",
    "                  .add(\"numberOfTrades\", \"integer\"))\n",
    "\n",
    "        s3_base_uri = \"s3a://deutsche-boerse-xetra-pds\"\n",
    "        if self.extraction_mode == \"full\":\n",
    "            s3_uri = f\"{s3_base_uri}/**/\"\n",
    "        elif self.extraction_mode == \"incremental\":\n",
    "            yesterday = (datetime.datetime.now(gettz('Berlin'))\n",
    "                         - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "            s3_uri = f\"{s3_base_uri}/{yesterday}/\"\n",
    "        else:\n",
    "            s3_uri = f\"{s3_base_uri}/{self.month_to_extract}*/\"\n",
    "\n",
    "        return self.spark.read.csv(s3_uri, schema=schema, header=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_datetime_column(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Return a new dataframe with column \"datetime\" added.\n",
    "\n",
    "        Column \"datetime\" is of data type \"timestamp\". The column is required to sort xetra trading\n",
    "        records.\n",
    "\n",
    "        Args:\n",
    "            df: xetra Spark DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            A Spark DataFrame\n",
    "        \"\"\"\n",
    "        return (df.withColumn(\"datetime\",\n",
    "                              functions.to_timestamp(functions.concat_ws(\" \", df.date, df.time))))\n",
    "\n",
    "    def get_opening_and_closing_prices(self, df: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
    "        \"\"\"Return two DataFrames with daily opening and closing prices for all trading securities.\n",
    "\n",
    "        Args:\n",
    "            df: A xetra Spark DataFrame\n",
    "\n",
    "        Returns:\n",
    "            A tuple of two DataFrames, first hold opening price info, the second holds the closing\n",
    "              price info.\n",
    "        \"\"\"\n",
    "        df_i = self._create_datetime_column(df)\n",
    "\n",
    "        asc_window = Window.partitionBy([\"isin\", \"date\"]).orderBy(df_i.datetime.asc())\n",
    "        start_df = (df_i.withColumn(\"asc_rank\", functions.rank().over(asc_window))\n",
    "                    .where(\"asc_rank = 1\")\n",
    "                    .drop(\"asc_rank\"))\n",
    "\n",
    "        desc_window = Window.partitionBy([\"isin\", \"date\"]).orderBy(df_i.datetime.desc())\n",
    "        end_df = (df_i.withColumn(\"desc_rank\", functions.rank().over(desc_window))\n",
    "                  .where(\"desc_rank = 1\")\n",
    "                  .drop(\"desc_rank\"))\n",
    "\n",
    "        return start_df, end_df\n",
    "\n",
    "    def get_intra_day_performance(self, opening: DataFrame, closing: DataFrame):\n",
    "        \"\"\" Get daily performance for each security.\n",
    "\n",
    "        Determine security performance by joining opening price with closing price.\n",
    "\n",
    "        Note that the current day data are dropped, because the complete CSVs will only be available\n",
    "        after the current day.\n",
    "\n",
    "        Args:\n",
    "            opening: A DataFrame with security daily opening price info\n",
    "            closing: A DataFrame with security daily closing price info\n",
    "\n",
    "        Returns:\n",
    "            A DataFrame of security daily performance. Each security has one row per trading day.\n",
    "        \"\"\"\n",
    "        df = (opening.alias(\"start\")\n",
    "                        .join(closing.alias(\"end\"), on=[\"isin\", \"date\"])\n",
    "                        .select(\"isin\", \"date\", \"start.startPrice\", \"end.endPrice\"))\n",
    "        if self.extraction_mode in [\"full\", \"month\"]:\n",
    "            today = datetime.datetime.now(gettz('Berlin')).strftime(\"%Y-%m-%d\")\n",
    "            df = df.where(f\"date != '{today}'\")\n",
    "        return (df\n",
    "                .withColumn(\"performance\", (df.endPrice - df.startPrice) / df.startPrice))\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_partition_column(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Return a new dataframe with column \"partition\" added.\n",
    "\n",
    "        Column \"partition\" is of data type \"string\". The column is used to write the xetra dataframe to\n",
    "        a Delta table out in partition.\n",
    "\n",
    "        Args:\n",
    "            df: xetra Spark DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            A Spark DataFrame\n",
    "        \"\"\"\n",
    "        return df.withColumn(\"partition\", functions.date_format(\"date\", \"yyyy-MM\"))\n",
    "        # .select(\"isin\", \"datetime\", \"date\", \"startPrice\", \"endPrice\", \"partition\")\n",
    "\n",
    "    def load_delta_table_stock_performance(self, df: DataFrame) -> None:\n",
    "        \"\"\"Load dataframe into Delta table.\n",
    "\n",
    "        Args:\n",
    "            df: A spark dataframe to write out\n",
    "        \"\"\"\n",
    "        df_i = self._create_partition_column(df)\n",
    "\n",
    "        if self.extraction_mode == \"incremental\":\n",
    "            output_mode = \"append\"\n",
    "        else:\n",
    "            output_mode = \"overwrite\"\n",
    "\n",
    "        (df_i.coalesce(1) # to reduce number of parquet files\n",
    "         .write\n",
    "         .format(\"delta\")\n",
    "         .partitionBy(\"partition\")\n",
    "         .mode(output_mode)\n",
    "         .saveAsTable(\"stock_performance\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initial Full ETL\n",
    "Historical xetra data are extracted from S3 bucket, transformed and loaded as Delta table\n",
    "`stock_performance`.\n",
    "\n",
    "Initial full ETL only needs to be run once.\n",
    "\n",
    "The full ETL processes data until `today - 1 day`, today's data will be processed tomorrow by\n",
    "the incremental ETL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/15 00:48:51 WARN Utils: Your hostname, LMAC-XIATONGZHENG.local resolves to a loopback address: 127.0.0.1; using 192.168.178.34 instead (on interface en0)\n",
      "22/02/15 00:48:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/zheng/git_projects/mop_hamster/venv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/15 00:48:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "etl = StockETL(extraction_mode=\"month\", month_to_extract=\"2022-02\")\n",
    "raw_df = etl.extract_xetra_data()\n",
    "# raw_df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=======================================================> (50 + 1) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+--------+--------------------+\n",
      "|        isin|      date|startPrice|endPrice|         performance|\n",
      "+------------+----------+----------+--------+--------------------+\n",
      "|DE0005493365|2022-02-01|     394.0|   408.4|  0.0365482078590974|\n",
      "|DE0005493365|2022-02-02|     411.6|   398.8|-0.03109819757225107|\n",
      "|DE0005493365|2022-02-03|     396.0|   386.0|-0.02525252525252...|\n",
      "|DE0005493365|2022-02-04|     386.0|   382.8|-0.00829018706484...|\n",
      "|DE0005493365|2022-02-07|     385.0|   374.2|-0.02805191634537...|\n",
      "|DE0005493365|2022-02-08|     385.0|   370.8|-0.03688314858969156|\n",
      "|DE0005493365|2022-02-09|     373.2|   383.2|0.026795283153561472|\n",
      "|DE0005493365|2022-02-10|     381.8|   376.0|-0.01519116809431067|\n",
      "|DE0005493365|2022-02-11|     371.2|   372.8|0.004310278915198789|\n",
      "|DE0005493365|2022-02-14|     358.0|   365.2| 0.02011176594142807|\n",
      "+------------+----------+----------+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "opening_price_df, closing_price_df = etl.get_opening_and_closing_prices(df=raw_df)\n",
    "performance_df = etl.get_intra_day_performance(opening=opening_price_df, closing=closing_price_df)\n",
    "performance_df.where(\"isin = 'DE0005493365'\").sort(\"date\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "etl.load_delta_table_stock_performance(performance_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Daily Incremental ETL\n",
    "After running the full ETL once, the incremental ETL should be scheduled every day to process data\n",
    "from yesterday."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StockETL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m incremental_etl \u001B[38;5;241m=\u001B[39m \u001B[43mStockETL\u001B[49m(extraction_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mincremental\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m yesterday_raw_df \u001B[38;5;241m=\u001B[39m incremental_etl\u001B[38;5;241m.\u001B[39mextract_xetra_data()\n\u001B[1;32m      3\u001B[0m start_df, end_df \u001B[38;5;241m=\u001B[39m incremental_etl\u001B[38;5;241m.\u001B[39mget_opening_and_closing_prices(yesterday_raw_df)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'StockETL' is not defined"
     ]
    }
   ],
   "source": [
    "incremental_etl = StockETL(extraction_mode=\"incremental\")\n",
    "yesterday_raw_df = incremental_etl.extract_xetra_data()\n",
    "start_df, end_df = incremental_etl.get_opening_and_closing_prices(yesterday_raw_df)\n",
    "yesterday_performance_df = incremental_etl.get_intra_day_performance(start_df, end_df)\n",
    "incremental_etl.load_delta_table_stock_performance(yesterday_performance_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "spark = StockETL.configure_spark_session()\n",
    "df = spark.read.format(\"delta\").load(\"spark-warehouse/stock_performance\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+--------+--------------------+---------+\n",
      "|        isin|      date|startPrice|endPrice|         performance|partition|\n",
      "+------------+----------+----------+--------+--------------------+---------+\n",
      "|DE000A0BVU28|2022-02-14|      23.6|    23.4|-0.00847460846214604|  2022-02|\n",
      "|DE000A0HGQS8|2022-02-14|      2.24|     2.2|-0.01785712575121809|  2022-02|\n",
      "|DK0060336014|2022-02-14|     51.66|   51.66|                 0.0|  2022-02|\n",
      "|IE00B0M62S72|2022-02-14|    20.995|   21.05| 0.00261959493349354|  2022-02|\n",
      "|IE00B4K48X80|2022-02-14|     65.27|   65.28|1.532424811535710...|  2022-02|\n",
      "|IE00BGHQ0G80|2022-02-14|    29.625|   29.66|0.001181429448510...|  2022-02|\n",
      "|IE00BJP26D89|2022-02-14|    4.9972|  5.0044|0.001440759672240772|  2022-02|\n",
      "|LU1598690169|2022-02-14|    120.18|  120.38|0.001664145013599...|  2022-02|\n",
      "|LU1834988864|2022-02-14|     59.59|   59.24|-0.00587344308147...|  2022-02|\n",
      "|LU1861136247|2022-02-14|     88.83|   89.01|0.002026345845608...|  2022-02|\n",
      "|US00650F1093|2022-02-14|      13.7|    13.8|0.007299298019120962|  2022-02|\n",
      "|US46120E6023|2022-02-14|     242.8|   250.5| 0.03171333134868526|  2022-02|\n",
      "|US4627261005|2022-02-14|     55.86|   57.14| 0.02291440682619103|  2022-02|\n",
      "|US55354G1004|2022-02-14|     472.3|   471.1|-0.00254071928957...|  2022-02|\n",
      "|US7374461041|2022-02-14|      95.5|    95.5|                 0.0|  2022-02|\n",
      "|US8718291078|2022-02-14|      73.3|   72.94|-0.00491133145106...|  2022-02|\n",
      "|DE0006464506|2022-02-14|      26.9|    26.0|-0.03345723536405...|  2022-02|\n",
      "|DE000A0KRKA0|2022-02-14|    9.7395|   9.678|-0.00631445098342...|  2022-02|\n",
      "|DE000A0V9X66|2022-02-14|    7.5215|  7.4955|-0.00345676028794...|  2022-02|\n",
      "|DE000A1NZLR7|2022-02-14|     4.527|  4.5566|0.006538578254224...|  2022-02|\n",
      "+------------+----------+----------+--------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}